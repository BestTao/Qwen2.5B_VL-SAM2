# åœ¨test_Qwen5çš„åŸºç¡€ä¸Šï¼Œä¿®æ”¹ç‚¹æç¤ºçš„ç”Ÿæˆ, æ¯”å¦‚ä¸€ä¸ªç›®æ ‡å¯èƒ½æœ‰å¤šä¸ªç‚¹ï¼Œéœ€è¦ä½¿ç”¨2ï¼Œ3ï¼Œ4ï¼Œ5ï¼Œè€Œä¸å…¨æ˜¯1
# SAM2æ¥å—çš„labelsæ˜¯[1,1,1,1]çš„ï¼Œä¸èƒ½æ˜¯[2,3]
# ä¿®æ”¹äº†æç¤ºè¯æ¨¡æ¿ï¼Œåƒé—®ç»“æœå¤„ç†ï¼Œåˆå¹¶å‡½æ•°ã€‚
import json
import torch
from torch_mlu.utils.model_transfer import transfer
from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor
import numpy as np
import os
import time
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import cv2
from scipy.ndimage import gaussian_filter
# from sam2.build_sam import build_sam2,build_sam2_video_predictor
import torch_mlu
import torch_mlu.utils.gpu_migration#æ³¨æ„å‰ä¸‰è¡Œ
from sam2.build_sam import build_sam2_video_predictor
# torch.bfloat16=torch.float16
from tqdm import tqdm
import decord
import os
from qwen_vl_utils import process_vision_info  # å®˜æ–¹å·¥å…·å‡½æ•°
import re
from collections import defaultdict


def generate_points_with_qwen(model, processor, image, user_prompot):
    """
    ä½¿ç”¨ Qwen2.5-VL æ¨¡å‹ç”ŸæˆæŒ‡å®šå¯¹è±¡çš„è¾¹ç•Œæ¡†ï¼ˆç‚¹æç¤ºï¼‰ã€‚
    é€šè¿‡æ›´æ˜ç¡®çš„æç¤ºè¯ï¼Œæ—¨åœ¨è·å–è§„èŒƒçš„ JSON è¾“å‡ºã€‚

    Args:
        model: å·²åŠ è½½çš„Qwen2.5-VLæ¨¡å‹
        processor: å·²åŠ è½½çš„å¤„ç†å™¨
        image (PIL.Image): å¾…æ£€æµ‹çš„RGBå›¾åƒ
        user_prompot (str or list): éœ€è¦æ£€æµ‹çš„å¯¹è±¡ç±»åˆ«åˆ—è¡¨ï¼ˆå¦‚["car", "person"]ï¼‰æˆ–å•ä¸ªå­—ç¬¦ä¸²ã€‚

    Returns:
        list: Qwen-VL çš„åŸå§‹è¾“å‡ºåˆ—è¡¨ï¼Œé€šå¸¸åŒ…å«ä¸€ä¸ªå­—ç¬¦ä¸²ï¼ˆJSONæ ¼å¼ï¼‰ã€‚
    """

    # Ensure user_prompot is a string for the prompt
    if isinstance(user_prompot, list):
        # Join multiple prompts, e.g., ["car", "person"] -> "caræˆ–person"
        user_prompot_str = "ã€".join(user_prompot)
    else:
        user_prompot_str = user_prompot

    # # Create messages
    # messages = [
    #     {
    #         "role": "user",
    #         "content": [
    #             {"type": "image", "image": image},
    #             {
    #                 "type": "text",
    #                 "text": f"è¯·ä»å›¾ç‰‡ä¸­æ£€æµ‹æ‰€æœ‰çš„{user_prompot}ã€‚å¯¹äºæ¯ä¸€ä¸ªæ£€æµ‹åˆ°çš„{user_prompot}ï¼Œæ ¹æ®ç›®æ ‡å¤§å°ï¼Œè¯·ç”Ÿæˆ **1åˆ°5ä¸ªä»£è¡¨å…¶å®Œæ•´è½®å»“çš„ç‚¹**ã€‚è¿™äº›ç‚¹åº”è¯¥åˆ†å¸ƒåœ¨ç›®æ ‡çš„å„ä¸ªéƒ¨åˆ†ï¼Œä¾‹å¦‚è½¦å¤´ã€è½¦èº«å’Œè½¦å°¾ç­‰ã€‚å°†ç»“æœä»¥ JSON æ ¼å¼è¾“å‡ºï¼Œå…¶ä¸­æ¯ä¸ªå¯¹è±¡çš„ 'points' é”®çš„å€¼å¿…é¡»æ˜¯ä¸€ä¸ªåŒ…å«ç‚¹åæ ‡çš„**åˆ—è¡¨çš„åˆ—è¡¨**ï¼Œä¾‹å¦‚ï¼š[{{\"points\": [[x1, y1], [x2, y2]], \"label\": \"label_name\"}}, {{ \"points\": [[x3, y3], [x4, y4], [x5, y5]], \"label\": \"label_name\"}}]"
    #             },
    #         ],
    #     }
    # ]
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image", "image": image},
                {
                    "type": "text",
                    "text": f"è¯·ä»å›¾ç‰‡ä¸­æ£€æµ‹æ‰€æœ‰çš„{user_prompot_str}ã€‚\n"
                              "å¯¹äºæ¯ä¸€ä¸ªæ£€æµ‹åˆ°çš„ç›®æ ‡ï¼Œè¯·ç”Ÿæˆ **3åˆ°5ä¸ªä»£è¡¨å…¶å®Œæ•´è½®å»“çš„å…³é”®ç‚¹**ã€‚\n"
                              "è¿™äº›ç‚¹åº”è¯¥åˆ†å¸ƒåœ¨ç›®æ ‡çš„å„ä¸ªéƒ¨åˆ†ï¼Œä¾‹å¦‚æ±½è½¦çš„å››ä¸ªè§’å’Œä¸­å¿ƒã€‚\n"
                              "è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON æ•°ç»„æ ¼å¼è¾“å‡ºç»“æœã€‚**ä¸è¦æœ‰ä»»ä½•é¢å¤–æ–‡å­—ï¼Œåªè¿”å› JSONã€‚**\n"
                              "**æ¯ä¸ª JSON å¯¹è±¡å¿…é¡»ç²¾ç¡®åœ°åŒ…å« 'points' å’Œ 'label' ä¸¤ä¸ªé”®ã€‚**\n"
                              "   - 'points' çš„å€¼å¿…é¡»æ˜¯ä¸€ä¸ªåŒ…å« `[x, y]` åæ ‡å¯¹çš„åˆ—è¡¨ï¼Œä¾‹å¦‚ `[[x1, y1], [x2, y2], [x3, y3]]`ã€‚\n"
                              "     **æ¯ä¸ªç‚¹å¿…é¡»æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„ `[x, y]` å­åˆ—è¡¨ï¼Œå³ä½¿åªæœ‰ä¸€ä¸ªç‚¹ã€‚**\n"
                              "   - 'label' çš„å€¼æ˜¯å¯¹è±¡çš„ç±»åˆ«å­—ç¬¦ä¸²ï¼Œä¾‹å¦‚ `\"car\"` æˆ– `\"road\"`ï¼Œ**ä¸è¦åŒ…å«é¢å¤–çš„æ–¹æ‹¬å·æˆ–å¼•å·ã€‚**\n"
                              "ä»¥ä¸‹æ˜¯ä¸¥æ ¼çš„ JSON ç¤ºä¾‹ï¼ˆè¯·ä¸¥æ ¼éµå¾ªæ­¤æ ¼å¼ï¼ŒåŒ…æ‹¬æ‰€æœ‰é€—å·å’Œæ‹¬å·ï¼‰ï¼š\n"
                              "```json\n"
                              "[\n"
                              "  {\"points\": [[100, 200], [150, 250], [100, 250], [150, 200]], \"label\": \"car\"},\n"
                              "  {\"points\": [[300, 400]], \"label\": \"road\"}  \n" # Example with a single point (still as [[x,y]])
                              "]\n"
                              "```\n"
                              "**å†æ¬¡å¼ºè°ƒï¼šåœ¨ 'points' åˆ—è¡¨ç»“æŸçš„ `]` ä¹‹åï¼Œå¿…é¡»ç´§è·Ÿä¸€ä¸ªé€—å· `,`ï¼Œç„¶åæ‰æ˜¯ 'label' é”®ã€‚**"
                },
            ],
        }
    ]    
    # Execute inference
    
    text = processor.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )
    # Assuming process_vision_info is defined elsewhere and works correctly
    # You'll need to make sure this function is correctly imported or defined.
    # For now, I'll put a placeholder if it's not provided.
    try:
        image_inputs, video_inputs = process_vision_info(messages)
    except NameError:
        print("Warning: 'process_vision_info' not found. Assuming simple image input structure.")
        # Fallback if process_vision_info is not provided
        # This part depends heavily on how your 'messages' are structured for processor
        # For typical single image input, it might look like this:
        image_inputs = [image] # Pass the PIL image directly if processor handles it
        video_inputs = None

    inputs = processor(
        text=[text],
        images=image_inputs,
        videos=video_inputs,
        padding=True,
        return_tensors="pt",
    ).to(model.device)

    # Generate output
    print("------------------Qwen2.5b_vlæ­£åœ¨ç”Ÿæˆè¾“å‡º------------------")
    generated_ids = model.generate(
        **inputs,
        max_new_tokens=2048
        
        # Potentially add temperature=0.1 or top_p=0.9 to encourage more deterministic output
        # num_beams=1 # Or increase for more diverse, but potentially slower, generation
    )

    # Decode generated IDs
    generated_ids_trimmed = [
        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
    ]
    return processor.batch_decode(
        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
    )


def pro_vl_results(rl_output):
    """
    å¤„ç† Qwen-VL çš„åŸå§‹è¾“å‡ºï¼Œæå– JSON å†…å®¹ï¼Œå¹¶å°†å…¶æ ¼å¼åŒ–ä¸º SAM2 æ‰€éœ€çš„ç‚¹æç¤ºã€‚
    å¢å¼ºäº†å¯¹ 'points' åµŒå¥—ç»“æ„ã€'label' å­—ç¬¦ä¸²åŒ–åˆ—è¡¨çš„é²æ£’æ€§ï¼Œå¹¶è¿‡æ»¤æ‰è´Ÿæ•°åæ ‡ç‚¹ã€‚

    Args:
        rl_output (list): Qwen-VL æ¨¡å‹çš„åŸå§‹è¾“å‡ºåˆ—è¡¨ï¼Œé€šå¸¸åŒ…å«ä¸€ä¸ªå­—ç¬¦ä¸²ã€‚

    Returns:
        dict: åŒ…å« 'points' å’Œ 'label' é”®çš„å­—å…¸ã€‚
              'points' æ˜¯ä¸€ä¸ªåŒ…å«æ‰€æœ‰å¯¹è±¡ç‚¹çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå­åˆ—è¡¨æ˜¯ [[x, y], ...]ã€‚
              'label' æ˜¯ä¸€ä¸ªåŒ…å«æ¯ä¸ªç‚¹é›†å¯¹åº”æ ‡ç­¾é•¿åº¦çš„åˆ—è¡¨ï¼Œä¾‹å¦‚ [[N1], [N2], ...]ã€‚
    """
    processed_points_for_sam2 = {'points': [], 'label': []}
    
    # 1. æå– JSON å†…å®¹
    json_content_str = None
    if rl_output and isinstance(rl_output[0], str):
        json_pattern = r'```json\n(.*?)\n```'
        json_match = re.search(json_pattern, rl_output[0], re.DOTALL)
        
        if json_match:
            extracted_json_content = json_match.group(1)
            # å°è¯•ä¿®å¤ ']]"label"' å˜æˆ ']], "label"' çš„æƒ…å†µ
            repaired_json_content = re.sub(r'\]\]\s*"label"', r']], "label"', extracted_json_content)
            repaired_json_content = re.sub(r'\]\s*"label"', r'], "label"', repaired_json_content)
            
            try:
                raw_point_results = json.loads(repaired_json_content)
            except json.JSONDecodeError as e:
                print(f"è­¦å‘Š: æå–çš„ JSON å†…å®¹è§£æå¤±è´¥: {e}. å†…å®¹: {repaired_json_content[:200]}...")
                raw_point_results = []
        else:
            # å¦‚æœæ²¡æœ‰ ```json``` æ ‡è®°ï¼Œå°è¯•ç›´æ¥è§£æï¼Œå°½ç®¡Qwen-VLé€šå¸¸ä¼šå¸¦
            try:
                raw_point_results = json.loads(rl_output[0])
            except json.JSONDecodeError:
                print("æœªåœ¨ ```json æ ‡è®°ä¹‹é—´æˆ–ä½œä¸ºçº¯ JSON å­—ç¬¦ä¸²æ‰¾åˆ° JSON å†…å®¹ã€‚")
                raw_point_results = []
    else:
        print(f"è­¦å‘Š: Qwen-VL è¾“å‡ºæ ¼å¼å¼‚å¸¸: {rl_output}. æœŸæœ›ä¸€ä¸ªåŒ…å«å­—ç¬¦ä¸²çš„åˆ—è¡¨ã€‚")
        raw_point_results = []

    # ç¡®ä¿ raw_point_results æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œä»¥ä¾¿ç»Ÿä¸€å¤„ç†
    if not isinstance(raw_point_results, list):
        raw_point_results = [raw_point_results]

    # 2. éå†è§£æåçš„ç»“æœï¼Œæå–å¹¶æ ¼å¼åŒ–ç‚¹ä¿¡æ¯
    for obj_data in raw_point_results:
        if 'points' in obj_data and isinstance(obj_data['points'], list):
            current_obj_raw_points = obj_data['points']
            
            # --- æ‰å¹³åŒ– points åˆ—è¡¨ START ---
            flattened_points = []
            def flatten(items):
                """é€’å½’æ‰å¹³åŒ–åˆ—è¡¨ï¼Œç›´åˆ°é‡åˆ°éåˆ—è¡¨å…ƒç´ æˆ–[x, y]å¯¹"""
                for item in items:
                    if isinstance(item, list) and len(item) == 2 and all(isinstance(coord_val, (int, float)) for coord_val in item):
                        # --- æ–°å¢çš„è´Ÿæ•°åæ ‡è¿‡æ»¤é€»è¾‘ START ---
                        if item[0] >= 0 and item[1] >= 0: # ç¡®ä¿ x å’Œ y åæ ‡éƒ½éè´Ÿ
                            # æ‰¾åˆ°äº†ä¸€ä¸ª [x, y] å¯¹ï¼Œä¸”åæ ‡æœ‰æ•ˆï¼Œç›´æ¥æ·»åŠ 
                            flattened_points.append(item)
                        else:
                            print(f"è­¦å‘Š: å‘ç°è´Ÿæ•°æˆ–æ— æ•ˆåæ ‡ï¼Œå·²è¿‡æ»¤: {item}")
                        # --- æ–°å¢çš„è´Ÿæ•°åæ ‡è¿‡æ»¤é€»è¾‘ END ---
                    elif isinstance(item, list):
                        # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œç»§ç»­é€’å½’æ‰å¹³åŒ–
                        flatten(item)
                    # å¦åˆ™ï¼Œè·³è¿‡éåˆ—è¡¨æˆ–é [x, y] çš„é¡¹ï¼Œæˆ–å¯ä»¥æ·»åŠ è­¦å‘Š
            
            flatten(current_obj_raw_points)
            # --- æ‰å¹³åŒ– points åˆ—è¡¨ END ---

            # --- è§£æ label å­—æ®µ START ---
            extracted_label = "unknown" # é»˜è®¤å€¼
            raw_label = obj_data.get('label', '')
            if isinstance(raw_label, str):
                # å°è¯•è§£æ "['road']" æˆ– "road"
                try:
                    # å°†å•å¼•å·æ›¿æ¢ä¸ºåŒå¼•å·ä»¥ä¾¿JSONè§£æ
                    parsed_label_list = json.loads(raw_label.replace("'", '"')) 
                    if isinstance(parsed_label_list, list) and len(parsed_label_list) > 0:
                        extracted_label = str(parsed_label_list[0]) # å–åˆ—è¡¨çš„ç¬¬ä¸€ä¸ªå…ƒç´ 
                    else:
                        extracted_label = raw_label # å¦‚æœè§£æå¤±è´¥æˆ–ä¸æ˜¯åˆ—è¡¨ï¼Œå°±ç”¨åŸå§‹å­—ç¬¦ä¸²
                except json.JSONDecodeError:
                    extracted_label = raw_label # å¦‚æœä¸æ˜¯æœ‰æ•ˆçš„JSONå­—ç¬¦ä¸²ï¼Œå°±ç”¨åŸå§‹å­—ç¬¦ä¸²
            elif raw_label is not None:
                # å¦‚æœ label ç›´æ¥æ˜¯å­—ç¬¦ä¸²ï¼ˆå¦‚ "road"ï¼‰ï¼Œæˆ–å…¶ä»–éå­—ç¬¦ä¸²ç±»å‹
                extracted_label = str(raw_label)
            # --- è§£æ label å­—æ®µ END ---

            if flattened_points: # å¦‚æœå½“å‰å¯¹è±¡æœ‰æœ‰æ•ˆçš„ç‚¹
                processed_points_for_sam2['points'].append(flattened_points)
                # SAM2é€šå¸¸æœŸæœ›æ¯ä¸ªç‚¹ä¸€ä¸ªæ ‡ç­¾ï¼Œä¸”éƒ½æ˜¯å‰æ™¯ï¼ˆ1ï¼‰
                # æˆ‘ä»¬è¿™é‡Œå­˜å‚¨çš„æ˜¯è¯¥ç‚¹é›†åŒ…å«çš„ç‚¹æ•°é‡ï¼Œåœ¨merge_annotationsä¸­ä¼šè½¬åŒ–ä¸º1
                processed_points_for_sam2['label'].append([len(flattened_points)])
            else:
                print(f"è­¦å‘Š: å¯¹è±¡æ•°æ®ä¸­ 'points' å­—æ®µä¸ºç©ºæˆ–æ— æ•ˆï¼Œæˆ–è€…æœªèƒ½æˆåŠŸæ‰å¹³åŒ–: {obj_data}")
        else:
            print(f"è­¦å‘Š: å¯¹è±¡æ•°æ®ä¸­ç¼ºå°‘ 'points' é”®æˆ–å…¶æ ¼å¼ä¸æ­£ç¡®: {obj_data}")

    return processed_points_for_sam2

# def pro_vl_results_7B(rl_output):
#     """
#     å¤„ç† Qwen-VL çš„åŸå§‹è¾“å‡ºï¼Œæå– JSON å†…å®¹ï¼Œå¹¶å°†å…¶æ ¼å¼åŒ–ä¸º SAM2 æ‰€éœ€çš„ç‚¹æç¤ºã€‚

#     Args:
#         rl_output (list): Qwen-VL æ¨¡å‹çš„åŸå§‹è¾“å‡ºåˆ—è¡¨ï¼Œé€šå¸¸åŒ…å«ä¸€ä¸ªå­—ç¬¦ä¸²ã€‚

#     Returns:
#         dict: åŒ…å« 'points' å’Œ 'label' é”®çš„å­—å…¸ï¼Œæ ¼å¼ä¸º
#               {'points': [[[x1, y1], [x2, y2], ...], [[x3, y3], ...]], 'label': [[N1], [N2], ...]}
#               å…¶ä¸­ N1, N2 æ˜¯å¯¹åº”å¯¹è±¡çš„ç‚¹æ•°é‡ã€‚
#     """
#     processed_points_for_sam2 = {'points': [], 'label': []}
#     json_content_str = None

#     # 1. å°è¯•ç›´æ¥è§£æ JSONï¼Œå¦‚æœ Qwen-VL è¾“å‡ºäº†ä¸€ä¸ªåˆæ³•çš„ JSON å­—ç¬¦ä¸²
#     if rl_output and isinstance(rl_output[0], str):
#         try:
#             # å°è¯•ä»å­—ç¬¦ä¸²ä¸­ç›´æ¥åŠ è½½ JSONã€‚
#             raw_point_results = json.loads(rl_output[0])
#             json_content_str = rl_output[0] # æ ‡è®°ä¸ºå·²æˆåŠŸè§£æ
#         except json.JSONDecodeError:
#             # å¦‚æœç›´æ¥åŠ è½½å¤±è´¥ï¼Œå°è¯•ç”¨æ­£åˆ™è¡¨è¾¾å¼æå– ```json``` å—
#             json_pattern = r'```json\n(.*?)\n```'
#             json_match = re.search(json_pattern, rl_output[0], re.DOTALL)
#             if json_match:
#                 json_content_str = json_match.group(1)
#                 try:
#                     raw_point_results = json.loads(json_content_str)
#                 except json.JSONDecodeError as e:
#                     print(f"è­¦å‘Š: æå–çš„ JSON å†…å®¹è§£æå¤±è´¥: {e}. å†…å®¹: {json_content_str[:200]}...")
#                     raw_point_results = []
#             else:
#                 print("æœªåœ¨ ```json æ ‡è®°ä¹‹é—´æˆ–ä½œä¸ºçº¯ JSON å­—ç¬¦ä¸²æ‰¾åˆ° JSON å†…å®¹ã€‚")
#                 raw_point_results = []
#     else:
#         print(f"è­¦å‘Š: Qwen-VL è¾“å‡ºæ ¼å¼å¼‚å¸¸: {rl_output}. æœŸæœ›ä¸€ä¸ªåŒ…å«å­—ç¬¦ä¸²çš„åˆ—è¡¨ã€‚")
#         raw_point_results = []

#     # ç¡®ä¿ raw_point_results æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œä»¥ä¾¿ç»Ÿä¸€å¤„ç†
#     if not isinstance(raw_point_results, list):
#         raw_point_results = [raw_point_results]

#     # 2. éå†è§£æåçš„ç»“æœï¼Œæå–ç‚¹ä¿¡æ¯
#     for obj_data in raw_point_results:
#         if 'points' in obj_data and isinstance(obj_data['points'], list):
#             # Qwen-VL çš„è¾“å‡ºä¸­ 'points' å­—æ®µæ˜¯ä¸€ä¸ªåŒ…å«å¤šä¸ªç‚¹åˆ—è¡¨çš„åˆ—è¡¨
#             # ä¾‹å¦‚: [[[x1, y1], [x2, y2]], [[x3, y3]]]
#             for point_list_for_an_object in obj_data['points']:
#                 # ç¡®ä¿ point_list_for_an_object æ˜¯ä¸€ä¸ªåŒ…å« [x, y] å¯¹çš„åˆ—è¡¨
#                 # æˆ‘ä»¬ä¸å†éœ€è¦ flatten_points å‡½æ•°ï¼Œå› ä¸º SAM-2 æœŸæœ›çš„æ ¼å¼å°±æ˜¯åµŒå¥—çš„
#                 # ç›´æ¥ä½¿ç”¨è¿™ä¸ªå†…éƒ¨åˆ—è¡¨ä½œä¸º SAM-2 çš„ä¸€ä¸ªç‚¹é›†
                
#                 # ç®€å•çš„æ ¡éªŒï¼Œç¡®ä¿åˆ—è¡¨ä¸­çš„å…ƒç´ æ˜¯ [x, y] å½¢å¼
#                 valid_points = []
#                 for p in point_list_for_an_object:
#                     if isinstance(p, list) and len(p) == 2 and all(isinstance(coord, (int, float)) for coord in p):
#                         valid_points.append(p)
                
#                 if valid_points: # å¦‚æœå½“å‰å¯¹è±¡æœ‰æœ‰æ•ˆçš„ç‚¹
#                     # å°†å½“å‰å¯¹è±¡çš„ç‚¹åˆ—è¡¨ä½œä¸º SAM-2 çš„ä¸€ä¸ªç‹¬ç«‹ç‚¹é›†æ·»åŠ 
#                     processed_points_for_sam2['points'].append(valid_points)
#                     # ä¸ºè¯¥ç‚¹é›†æ·»åŠ å¯¹åº”çš„æ ‡ç­¾ï¼Œå³è¯¥ç‚¹é›†çš„ç‚¹æ•°é‡
#                     processed_points_for_sam2['label'].append([len(valid_points)])
#         else:
#             print(f"è­¦å‘Š: å¯¹è±¡æ•°æ®ä¸­ç¼ºå°‘ 'points' é”®æˆ–å…¶æ ¼å¼ä¸æ­£ç¡®: {obj_data}")

#     # print(f"ä¸º SAM-2 æ ¼å¼åŒ–åçš„ç‚¹å’Œæ ‡ç­¾: {processed_points_for_sam2}")
#     return processed_points_for_sam2


import json
import re

def pro_vl_results_32B(rl_output):
    """
    å¤„ç† Qwen-VL çš„åŸå§‹è¾“å‡ºï¼Œæå– JSON å†…å®¹ï¼Œå¹¶å°†å…¶æ ¼å¼åŒ–ä¸º SAM2 æ‰€éœ€çš„ç‚¹æç¤ºã€‚
    å¢å¼ºäº†å¯¹ 'points' åµŒå¥—ç»“æ„ã€'label' å­—ç¬¦ä¸²åŒ–åˆ—è¡¨çš„é²æ£’æ€§ï¼Œè¿‡æ»¤è´Ÿæ•°åæ ‡ï¼Œ
    å¹¶æ›´çµæ´»åœ°ä¿®å¤ JSON æ ¼å¼ä¸­ç¼ºå¤±çš„é€—å·ã€‚

    Args:
        rl_output (list): Qwen-VL æ¨¡å‹çš„åŸå§‹è¾“å‡ºåˆ—è¡¨ï¼Œé€šå¸¸åŒ…å«ä¸€ä¸ªå­—ç¬¦ä¸²ã€‚

    Returns:
        dict: åŒ…å« 'points' å’Œ 'label' é”®çš„å­—å…¸ã€‚
              'points' æ˜¯ä¸€ä¸ªåŒ…å«æ‰€æœ‰å¯¹è±¡ç‚¹çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå­åˆ—è¡¨æ˜¯ [[x, y], ...]ã€‚
              'label' æ˜¯ä¸€ä¸ªåŒ…å«æ¯ä¸ªç‚¹é›†å¯¹åº”æ ‡ç­¾é•¿åº¦çš„åˆ—è¡¨ï¼Œä¾‹å¦‚ [[N1], [N2], ...]ã€‚
    """
    processed_points_for_sam2 = {'points': [], 'label': []}
    # print(f"ğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜Š{rl_output[0]}")
    # 1. æå– JSON å†…å®¹
    json_content_str = None
    if rl_output and isinstance(rl_output[0], str):
        json_pattern = r'```json\n(.*?)\n```'
        json_match = re.search(json_pattern, rl_output[0], re.DOTALL)
        
        if json_match:
            extracted_json_content = json_match.group(1)
            
            # --- æ›´é€šç”¨çš„ JSON ä¿®å¤é€»è¾‘ START ---
            # ä¿®å¤ '[...]"label"' å˜æˆ '[...], "label"' çš„æƒ…å†µ
            # åŒ¹é…ä»»ä½•ä¸€ä¸ªæˆ–å¤šä¸ª ']' åé¢ç›´æ¥è·Ÿç€ä¸€ä¸ª '"' (å¦‚æœå‰é¢ä¸æ˜¯ ',')
            # è¿™æ˜¯ä¸€ä¸ªæ›´é€šç”¨çš„æ¨¡å¼ï¼Œä»¥æ•æ‰ ']]"label"' å’Œ ']"label"' ç”šè‡³å¯èƒ½æ˜¯ ']]]"label"'
            repaired_json_content = re.sub(r'\]\s*(?<!,)"', r'], "', extracted_json_content)
            # å†æ¬¡ä¿®å¤ï¼Œä»¥é˜²æœ‰åµŒå¥—çš„ ]] ä¸”ä¹‹å‰æ²¡æœ‰é€—å·ï¼Œä¾‹å¦‚ [[x,y]],"label" å˜æˆ [[x,y]],,"label"
            # ç¡®ä¿åªåœ¨å¿…è¦æ—¶æ·»åŠ é€—å·ï¼Œé¿å…é‡å¤
            repaired_json_content = re.sub(r'\]\s*,\s*\]\s*,\s*"', r']], "', repaired_json_content) # For cases like ']],,"label"' from previous repair
            
            repaired_json_content = re.sub(r'"label":\s*"\[\'(.*?)\'\]"', r'"label": "\1"', repaired_json_content)

            # --- æ›´é€šç”¨çš„ JSON ä¿®å¤é€»è¾‘ END ---
            
            try:
                raw_point_results = json.loads(repaired_json_content)
            except json.JSONDecodeError as e:
                print(f"è­¦å‘Š: æå–çš„ JSON å†…å®¹è§£æå¤±è´¥: {e}. å†…å®¹: {repaired_json_content[:200]}...")
                raw_point_results = []
        else:
            # If no ```json``` marker, try direct parsing, although Qwen-VL usually provides one
            try:
                raw_point_results = json.loads(rl_output[0])
            except json.JSONDecodeError:
                print("æœªåœ¨ ```json æ ‡è®°ä¹‹é—´æˆ–ä½œä¸ºçº¯ JSON å­—ç¬¦ä¸²æ‰¾åˆ° JSON å†…å®¹ã€‚")
                raw_point_results = []
    else:
        print(f"è­¦å‘Š: Qwen-VL è¾“å‡ºæ ¼å¼å¼‚å¸¸: {rl_output}. Expected a list containing a string.")
        raw_point_results = []

    # Ensure raw_point_results is a list for uniform processing
    if not isinstance(raw_point_results, list):
        raw_point_results = [raw_point_results]

    # 2. Iterate through parsed results, extract and format point information
    for obj_data in raw_point_results:
        if 'points' in obj_data and isinstance(obj_data['points'], list):
            current_obj_raw_points = obj_data['points']
            
            # --- Flatten points list START ---
            flattened_points = []
            def flatten(items):
                """Recursively flattens lists until non-list elements or [x, y] pairs are found."""
                for item in items:
                    # Check for [x, y] pair and non-negative coordinates
                    if isinstance(item, list) and len(item) == 2 and all(isinstance(coord_val, (int, float)) for coord_val in item):
                        if item[0] >= 0 and item[1] >= 0:
                            flattened_points.append(item)
                        else:
                            print(f"è­¦å‘Š: å‘ç°è´Ÿæ•°æˆ–æ— æ•ˆåæ ‡ï¼Œå·²è¿‡æ»¤: {item}")
                    elif isinstance(item, list):
                        flatten(item)
                    # Else, skip non-list or non-[x, y] items
            
            flatten(current_obj_raw_points)
            # --- Flatten points list END ---

            # --- Parse label field START ---
            extracted_label = "unknown" # Default value
            raw_label = obj_data.get('label', '')
            # If label is a string, try to clean it
            if isinstance(raw_label, str):
                # We tried to repair `["'road'"]` to `"road"` earlier with regex.
                # If that failed, this attempts to parse it as JSON
                try:
                    parsed_label_list = json.loads(raw_label.replace("'", '"')) 
                    if isinstance(parsed_label_list, list) and len(parsed_label_list) > 0:
                        extracted_label = str(parsed_label_list[0])
                    else:
                        extracted_label = raw_label 
                except json.JSONDecodeError:
                    extracted_label = raw_label # If not a valid JSON string, use original
            elif raw_label is not None:
                extracted_label = str(raw_label)
            # --- Parse label field END ---

            if flattened_points: # If current object has valid points
                processed_points_for_sam2['points'].append(flattened_points)
                # SAM2 usually expects one label per point, all foreground (1)
                # We store the number of points in this set; it will be converted to 1s in merge_annotations
                processed_points_for_sam2['label'].append([len(flattened_points)]) # Keep the length for now
            else:
                print(f"è­¦å‘Š: å¯¹è±¡æ•°æ®ä¸­ 'points' å­—æ®µä¸ºç©ºæˆ–æ— æ•ˆï¼Œæˆ–è€…æœªèƒ½æˆåŠŸæ‰å¹³åŒ–: {obj_data}")
        else:
            print(f"è­¦å‘Š: å¯¹è±¡æ•°æ®ä¸­ç¼ºå°‘ 'points' é”®æˆ–å…¶æ ¼å¼ä¸æ­£ç¡®: {obj_data}")

    return processed_points_for_sam2
def timeit(func):
    """Decorator to measure function execution time."""
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        elapsed_time = time.time() - start_time
        return result
    return wrapper

# def merge_annotations(annotations):
#     """
#     åˆå¹¶å…·æœ‰ç›¸åŒ frame_idx çš„æ³¨è§£ã€‚
#     å°†æ¯ä¸ª frame_idx ä¸‹çš„æ‰€æœ‰ç‚¹çš„ 'points' å’Œ 'labels' èšåˆèµ·æ¥ã€‚

#     Args:
#         annotations (list): åŸå§‹çš„æ³¨è§£åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« 'obj_id', 'frame_idx', 'points', 'labels', 'box'ã€‚

#     Returns:
#         list: åˆå¹¶åçš„æ³¨è§£åˆ—è¡¨ã€‚æ¯ä¸ªå…ƒç´ ä»£è¡¨ä¸€ä¸ª frame_idx ä¸‹çš„æ‰€æœ‰å¯¹è±¡ã€‚
#               å…¶ 'points' åŒ…å«æ‰€æœ‰å¯¹è±¡çš„æ‰å¹³åŒ–ç‚¹åˆ—è¡¨ï¼Œ'labels' åŒ…å«æ‰€æœ‰åŸå§‹å¯¹è±¡çš„ç‚¹æ•°é‡åˆ—è¡¨ã€‚
#               'obj_id' è¡¨ç¤ºè¯¥å¸§ä¸­åŸå§‹å¯¹è±¡çš„æ€»æ•°ã€‚
#     """
#     merged_by_frame = defaultdict(
#         lambda: {'points': [], 'labels': [], 'box': [], 'original_obj_count': 0}
#     )

#     for annot in annotations:
#         frame_idx = annot['frame_idx']
        
#         # å°†å½“å‰æ³¨è§£çš„ç‚¹æ·»åŠ åˆ°è¯¥å¸§çš„æ€»ç‚¹åˆ—è¡¨ä¸­ (æ‰å¹³åŒ–)
#         # ç¡®ä¿ annot['points'] æ˜¯ä¸€ä¸ªåˆ—è¡¨çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå­åˆ—è¡¨æ˜¯ [x, y]
#         # æ‚¨çš„è¾“å…¥ç¤ºä¾‹æ˜¯ï¼š'points': [[1600, 1080], [1790, 1080], [1850, 1080]]
#         # è¿™é‡Œç›´æ¥ extend å³å¯
#         merged_by_frame[frame_idx]['points'].extend(annot['points'])
        
#         # å°†åŸå§‹å¯¹è±¡çš„æ ‡ç­¾ï¼ˆç‚¹æ•°é‡ï¼‰æ·»åŠ åˆ°è¯¥å¸§çš„æ€»æ ‡ç­¾åˆ—è¡¨ä¸­
#         # ç¡®ä¿ annot['labels'] æ˜¯ä¸€ä¸ªåƒ [3] è¿™æ ·çš„åˆ—è¡¨
#         merged_by_frame[frame_idx]['labels'].extend(annot['labels'])
        
#         # å¢åŠ åŸå§‹å¯¹è±¡è®¡æ•°
#         merged_by_frame[frame_idx]['original_obj_count'] += 1
        
#         # æ¡†ä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼Œè¿™é‡Œç®€å•åœ°å–ç¬¬ä¸€ä¸ªï¼Œæˆ–è€…æ ¹æ®éœ€è¦è¿›è¡Œåˆå¹¶ï¼‰
#         # è€ƒè™‘åˆ°ä½ çš„ç¤ºä¾‹ä¸­ box éƒ½æ˜¯ç©ºåˆ—è¡¨ï¼Œè¿™é‡Œä¸åšå¤æ‚åˆå¹¶
#         if annot['box']:
#             # è¿™é‡Œçš„é€»è¾‘éœ€è¦æ ¹æ®ä½ å®é™…çš„ box å¤„ç†éœ€æ±‚æ¥å®š
#             # å¦‚æœæ˜¯å¤šä¸ª box éœ€è¦åˆå¹¶æˆä¸€ä¸ªå¤§ boxï¼Œéœ€è¦æ›´å¤æ‚çš„è®¡ç®—
#             # å¦‚æœåªæ˜¯ä¸ºäº†å ä½ï¼Œå¯ä»¥ä¿æŒç©º
#             pass # æˆ–è€… merged_by_frame[frame_idx]['box'].extend(annot['box'])

#     final_merged_annotations = []
#     for frame_idx in sorted(merged_by_frame.keys()):
#         merged_data = merged_by_frame[frame_idx]
#         final_merged_annotations.append({
#             'obj_id': merged_data['original_obj_count'], # 'obj_id'è¡¨ç¤ºè¯¥å¸§ä¸­åŸå§‹å¯¹è±¡çš„æ€»æ•°
#             'frame_idx': frame_idx,
#             'points': merged_data['points'], # åŒ…å«æ‰€æœ‰åŸå§‹å¯¹è±¡çš„æ‰å¹³åŒ–ç‚¹åˆ—è¡¨
#             'labels': merged_data['labels'], # åŒ…å«æ‰€æœ‰åŸå§‹å¯¹è±¡çš„ç‚¹æ•°é‡åˆ—è¡¨ï¼Œå¦‚ [3, 3, 3]
#             'box': merged_data['box'] # ä¿æŒä¸ºåŸå§‹æ”¶é›†çš„boxï¼ˆå¦‚æœå­˜åœ¨ï¼‰
#         })
#     return final_merged_annotations
def merge_annotations(annotations):
    """
    åˆå¹¶å…·æœ‰ç›¸åŒ frame_idx çš„æ³¨è§£ã€‚
    å°†æ¯ä¸ª frame_idx ä¸‹çš„æ‰€æœ‰ç‚¹çš„ 'points' å’Œ 'labels' èšåˆèµ·æ¥ã€‚
    ç”Ÿæˆçš„ 'labels' æ ¼å¼ä¸º [1, 1, ..., 1]ï¼Œæ•°é‡ä¸åˆå¹¶åçš„ç‚¹æ•°åŒ¹é…ã€‚

    Args:
        annotations (list): åŸå§‹çš„æ³¨è§£åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« 'obj_id', 'frame_idx', 'points', 'labels', 'box'ã€‚

    Returns:
        list: åˆå¹¶åçš„æ³¨è§£åˆ—è¡¨ã€‚æ¯ä¸ªå…ƒç´ ä»£è¡¨ä¸€ä¸ª frame_idx ä¸‹çš„æ‰€æœ‰å¯¹è±¡ã€‚
              å…¶ 'points' åŒ…å«æ‰€æœ‰å¯¹è±¡çš„æ‰å¹³åŒ–ç‚¹åˆ—è¡¨ï¼Œ'labels' åŒ…å«ä¸è¿™äº›ç‚¹ä¸€ä¸€å¯¹åº”çš„ '1' æ ‡ç­¾ã€‚
              'obj_id' è¡¨ç¤ºè¯¥å¸§ä¸­åŸå§‹å¯¹è±¡çš„æ€»æ•°ï¼ˆç”¨äºå…ƒæ•°æ®ï¼ŒSAM2å¯èƒ½ä¸ç›´æ¥ä½¿ç”¨æ­¤IDï¼‰ã€‚
    """
    merged_by_frame = defaultdict(
        lambda: {'points': [], 'labels': [], 'box': [], 'original_obj_count': 0}
    )

    for annot in annotations:
        frame_idx = annot['frame_idx']
        
        # å°†å½“å‰æ³¨è§£çš„ç‚¹æ·»åŠ åˆ°è¯¥å¸§çš„æ€»ç‚¹åˆ—è¡¨ä¸­ (æ‰å¹³åŒ–)
        # annot['points'] å·²ç»æ˜¯åƒ [[x1, y1], [x2, y2]] è¿™æ ·çš„åˆ—è¡¨
        merged_by_frame[frame_idx]['points'].extend(annot['points'])
        
        # æ ¹æ®å½“å‰æ³¨è§£çš„ç‚¹æ•°é‡ï¼Œæ·»åŠ ç›¸åº”æ•°é‡çš„ '1' åˆ°è¯¥å¸§çš„æ€»æ ‡ç­¾åˆ—è¡¨ä¸­
        # annot['points'] çš„é•¿åº¦å°±æ˜¯è¯¥åŸå§‹å¯¹è±¡æ‰€åŒ…å«çš„ç‚¹æ•°
        num_points_in_current_object = len(annot['points'])
        merged_by_frame[frame_idx]['labels'].extend([1] * num_points_in_current_object)
        
        # å¢åŠ åŸå§‹å¯¹è±¡è®¡æ•°
        merged_by_frame[frame_idx]['original_obj_count'] += 1
        
        # æ¡†ä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼Œè¿™é‡Œç®€å•åœ°å–ç¬¬ä¸€ä¸ªï¼Œæˆ–è€…æ ¹æ®éœ€è¦è¿›è¡Œåˆå¹¶ï¼‰
        if annot['box']:
            # è¿™é‡Œçš„é€»è¾‘éœ€è¦æ ¹æ®ä½ å®é™…çš„ box å¤„ç†éœ€æ±‚æ¥å®š
            pass # ä¿æŒä¸åŠ¨ï¼Œæˆ–æ ¹æ®ä½ çš„åˆå¹¶ç­–ç•¥å¤„ç†

    final_merged_annotations = []
    for frame_idx in sorted(merged_by_frame.keys()):
        merged_data = merged_by_frame[frame_idx]
        final_merged_annotations.append({
            'obj_id': merged_data['original_obj_count'], # æ­¤å¤„çš„ obj_id ä½œä¸ºå…ƒæ•°æ®ï¼Œè¡¨ç¤ºåŸå§‹å¯¹è±¡çš„æ€»æ•°
            'frame_idx': frame_idx,
            'points': merged_data['points'], # åŒ…å«æ‰€æœ‰åŸå§‹å¯¹è±¡çš„æ‰å¹³åŒ–ç‚¹åˆ—è¡¨
            'labels': merged_data['labels'], # åŒ…å«ä¸æ‰€æœ‰ç‚¹ä¸€ä¸€å¯¹åº”çš„ '1' æ ‡ç­¾ï¼Œå¦‚ [1, 1, 1, 1, 1, 1, 1, 1, 1]
            'box': merged_data['box'] # ä¿æŒä¸ºåŸå§‹æ”¶é›†çš„boxï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        })
    return final_merged_annotations

def init_models():
    """åˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹"""
    # SAM2
    SAM2_CHECKPOINT = "checkpoints/sam2.1_hiera_large.pt"
    SAM2_CONFIG = "configs/sam2.1/sam2.1_hiera_l.yaml"
    sam_predictor = build_sam2_video_predictor(SAM2_CONFIG, SAM2_CHECKPOINT)
    
    return sam_predictor

class ObjectTracker:
    def __init__(self, iou_threshold=0.3, max_missing_frames=30):
        self.objects = {}  # obj_id: {"last_box", "last_frame", "prompt"}
        self.next_id = 1
        self.iou_threshold = iou_threshold
        self.max_missing = max_missing_frames
    
    def update(self, detections, frame_idx):
        """æ›´æ–°è¿½è¸ªå™¨ï¼Œè¿”å›æ¯ä¸ªæ£€æµ‹å¯¹åº”çš„å¯¹è±¡ID"""
        self._cleanup(frame_idx)
        
        if not detections:
            return []
        
        # New: Initialize the list to store obj_id for each detection
        # This list will have the same length and order as 'detections'
        assigned_obj_ids = [None] * len(detections)
        
        # Keep track of which existing objects have been matched in this frame
        matched_existing_obj_ids = set()
        
        # 1. å°è¯•åŒ¹é…å·²æœ‰å¯¹è±¡
        for obj_id, obj_data in list(self.objects.items()): # Use list() to iterate over a copy if modifying self.objects
            best_iou = 0
            best_det_idx = -1
            
            for det_idx, det in enumerate(detections):
                # Skip if this detection has already been assigned an ID
                if assigned_obj_ids[det_idx] is not None:
                    continue
                
                # Only match objects with the same prompt (important for multi-object tracking)
                if det["prompt"] != obj_data["prompt"]:
                    continue
                
                iou = self._calculate_iou(obj_data["last_box"], det["box"])
                if iou > best_iou:
                    best_iou = iou
                    best_det_idx = det_idx
            
            if best_iou > self.iou_threshold:
                # Match successful: Assign the existing obj_id to this detection
                assigned_obj_ids[best_det_idx] = obj_id
                matched_existing_obj_ids.add(obj_id) # Mark existing obj as matched
                
                # Update object state with the new box and current frame
                self.objects[obj_id]["last_box"] = detections[best_det_idx]["box"]
                self.objects[obj_id]["last_frame"] = frame_idx
        
        # 2. å¤„ç†æœªåŒ¹é…çš„æ£€æµ‹ï¼ˆæ–°å¯¹è±¡ï¼‰
        for det_idx, det in enumerate(detections):
            if assigned_obj_ids[det_idx] is None: # If this detection has not been assigned an ID yet
                new_id = self.next_id
                self.next_id += 1
                self.objects[new_id] = {
                    "prompt": det["prompt"],
                    "last_box": det["box"],
                    "last_frame": frame_idx
                }
                assigned_obj_ids[det_idx] = new_id
        
        # 3. Update 'last_frame' for existing objects that were not matched in this frame
        # (This is implicitly handled by _cleanup, but explicitly setting for robustness if _cleanup logic changes)
        # However, the current cleanup handles it fine. Just ensuring all objects in self.objects have updated 'last_frame' or are marked for cleanup.
        # This loop is technically not needed if _cleanup handles it and assigned_obj_ids fully covers all 'detections'

        return assigned_obj_ids # Return the list with obj_ids for each detection

    def _cleanup(self, current_frame):
        """æ¸…ç†é•¿æ—¶é—´æœªå‡ºç°çš„å¯¹è±¡"""
        to_remove = []
        for obj_id, obj_data in list(self.objects.items()): # Iterate over a copy
            if current_frame - obj_data["last_frame"] > self.max_missing:
                to_remove.append(obj_id)
        
        for obj_id in to_remove:
            del self.objects[obj_id]
    
    def _calculate_iou(self, box1, box2):
        """è®¡ç®—ä¸¤ä¸ªæ¡†çš„IoU"""
        # ... (IoU calculation remains the same)
        x1_min, y1_min, x1_max, y1_max = box1
        x2_min, y2_min, x2_max, y2_max = box2
        
        inter_x_min = max(x1_min, x2_min)
        inter_y_min = max(y1_min, y2_min)
        inter_x_max = min(x1_max, x2_max)
        inter_y_max = min(y1_max, y2_max)
        
        inter_area = max(0, inter_x_max - inter_x_min) * max(0, inter_y_max - inter_y_min)
        
        area1 = (x1_max - x1_min) * (y1_max - y1_min)
        area2 = (x2_max - x2_min) * (y2_max - y2_min)
        
        union_area = area1 + area2 - inter_area
        return inter_area / union_area if union_area > 0 else 0

CONFIG = {
    "prompts": ["house"], # æç¤ºè¯
    "keyframe_interval": 20,   # æç¤ºå¸§çš„é¢‘ç‡
    "sam_config": "checkpoints/sam2.1_hiera_large.pt",
    "sam_checkpoint": "configs/sam2.1/sam2.1_hiera_l.yaml"
}

def main():
    # 0. é…ç½®
    video_file = "/workspace/volume/gxs2/zht/project/sam2-sam2.1_mlu/notebooks/videos/video2-12s.mp4"
    output_dir = "/workspace/volume/gxs2/zht/project/sam2-sam2.1_mlu/notebooks/videos/house"
    # 1. åˆå§‹åŒ–SAM2æ¨¡å‹
    sam_predictor= init_models()
    
    # 2. å‡†å¤‡è§†é¢‘
    vr = decord.VideoReader(video_file)
    total_frames = len(vr)
    # keyframe_indices = set()
    # keyframe_indices.add(0)
    # 3. åŠ è½½Qwenæ¨¡å‹ï¼ˆä»…éœ€åŠ è½½ä¸€æ¬¡ï¼‰
    # model_id = "Qwen/Qwen2.5-VL-7B-Instruct"
    qwen_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
        "/workspace/volume/gxs2/zht/Models/models/Qwen/Qwen2.5-VL-32B-Instruct",
        torch_dtype=torch.float16,
        attn_implementation="flash_attention_2",
        device_map="auto",
    )
    qwen_processor = AutoProcessor.from_pretrained(
        "/workspace/volume/gxs2/zht/Models/models/Qwen/Qwen2.5-VL-32B-Instruct"
        )

    # 4. ç¡®å®šå…³é”®å¸§
    keyframe_indices = set([0]) # ä¿æŒä¸ºsetç±»å‹
    
    # ç¬¬ä¸€æ¬¡æ·»åŠ å…³é”®å¸§
    for frame_idx in range(0, total_frames, CONFIG["keyframe_interval"]):
        keyframe_indices.add(frame_idx)
    # åœ¨æ‰€æœ‰æ·»åŠ æ“ä½œå®Œæˆåï¼Œå†å°† set è½¬æ¢ä¸ºæ’åºåçš„ list
    keyframe_indices = sorted(list(keyframe_indices)) # å…ˆè½¬æ¢ä¸ºlistå†æ’åº
    print(f"å…³é”®å¸§åˆ—è¡¨: {keyframe_indices}")

    # 5. æ”¶é›†æç¤ºç‚¹
    annotations = []
    tracked_objects = {}
    next_obj_id = 1
    object_tracker = ObjectTracker(
        iou_threshold=0.3,   # IoUåŒ¹é…é˜ˆå€¼
        max_missing_frames=30 # æœ€å¤§æ¶ˆå¤±å¸§æ•°
    )    
    
    for frame_idx in tqdm(keyframe_indices, desc="å¤„ç†å…³é”®å¸§"):
        # è·å–å¹¶å¤„ç†å¸§
        frame = vr[frame_idx].asnumpy()
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        frame_pil_from_array = Image.fromarray(frame_rgb)
        objects_to_detect = CONFIG["prompts"]
        # if frame_idx == 0:
        #     objects_to_detect = CONFIG["prompts"]
        # else:
        #     objects_to_detect = [obj["prompt"] for obj in tracked_objects.values()]

        
        # # image_path = "demo.jpeg"
        # # frame_pil = Image.open(image_path)
        print(f"objects_to_detect:{objects_to_detect}")
        # ä½¿ç”¨Qwenæ¨¡å‹ç”Ÿæˆç‚¹æç¤º
        vl_results = generate_points_with_qwen(
            model=qwen_model,
            processor=qwen_processor,
            image=frame_pil_from_array,
            user_prompot=objects_to_detect
        )
        print(f"vl_results:{vl_results}")
        # processed_points = pro_vl_results(vl_results)
        processed_points = pro_vl_results_32B(vl_results)
        print(f"å¤„ç†åçš„LLMç»“æœ:{processed_points}")

        # ç›´æ¥éå† processed_points['points'] ä¸­æ¯ä¸ªç‹¬ç«‹çš„ç‚¹é›†
        next_obj_id = 1 # ç¡®ä¿æ¯æ¬¡å¤„ç†æ–°å¸§æ—¶ï¼Œobj_idä»1å¼€å§‹é€’å¢ï¼Œæˆ–è€…æœ‰æ›´å®Œå–„çš„å…¨å±€IDç®¡ç†
        for i, point_set in enumerate(processed_points['points']):
            # point_set æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œä¾‹å¦‚ [[1600, 1080], [1790, 1080], [1850, 1080]]
            # è¿™é‡Œçš„ obj_id éœ€è¦è€ƒè™‘æ˜¯é’ˆå¯¹å½“å‰å¸§çš„æ–°å¯¹è±¡ï¼Œè¿˜æ˜¯è·¨å¸§è¿½è¸ªçš„æ—§å¯¹è±¡
            # å¦‚æœä¸è¿›è¡Œè·¨å¸§è·Ÿè¸ªï¼ˆå¦‚æ‚¨æ‰€è¯´ï¼ŒQwen-VLé¿å…åŒä¸€å¯¹è±¡ï¼‰ï¼Œåˆ™æ¯æ£€æµ‹åˆ°ä¸€ä¸ªæ–°ç‚¹é›†ï¼Œå°±ç»™ä¸€ä¸ªæ–°ID
            obj_id = next_obj_id
            next_obj_id += 1 # æ¯æ¬¡æ£€æµ‹åˆ°ä¸€ä¸ªæ–°çš„å¯¹è±¡ï¼ˆç‚¹é›†ï¼‰å°±é€’å¢ID
            
            # å‡è®¾ Qwen-VL è¿”å›çš„ processed_points['label'] ä¸ 'points' çš„å­åˆ—è¡¨ä¸€ä¸€å¯¹åº”
            # ä¾‹å¦‚ processed_points['label'][i] å¯¹åº” processed_points['points'][i]
            label_for_this_set = processed_points['label'][i] # åº”è¯¥æ˜¯ä¸€ä¸ªåƒ [[3]] è¿™æ ·çš„åˆ—è¡¨

            annotations.append({
                "obj_id": obj_id, # ä¸ºæ¯ä¸ªQwen-VLæ£€æµ‹åˆ°çš„å¯¹è±¡ï¼ˆç‚¹é›†ï¼‰åˆ†é…ä¸€ä¸ªç‹¬ç«‹çš„ID
                "frame_idx": frame_idx,
                "points": point_set, # è¿™æ˜¯ä¸€ä¸ªåŒ…å«å¤šä¸ª [x, y] çš„åˆ—è¡¨
                "labels": label_for_this_set, # è¿™æ˜¯è¯¥ç‚¹é›†å¯¹åº”çš„æ ‡ç­¾ï¼Œä¾‹å¦‚ [[3]]
                "box": []  # æ²¡æœ‰è¾¹ç•Œæ¡†ä¿¡æ¯ï¼Œä¿æŒä¸ºç©ºåˆ—è¡¨
            })

    # annotations = [{'obj_id': 1, 'frame_idx': 0, 'points': [[1568, 267]], 'labels': [[1]], 'box': [1540, 223, 1596, 311]}, {'obj_id': 1, 'frame_idx': 0, 'points': [[1751, 85]], 'labels': [[1]], 'box': [1728, 46, 1775, 125]}, {'obj_id': 1, 'frame_idx': 0, 'points': [[2256, 40]], 'labels': [[1]], 'box': [2240, 29, 2272, 52]}, {'obj_id': 1, 'frame_idx': 50, 'points': [[1673, 273]], 'labels': [[1]], 'box': [1645, 239, 1702, 308]}, {'obj_id': 1, 'frame_idx': 50, 'points': [[1830, 518]], 'labels': [[1]], 'box': [1808, 499, 1852, 538]}, {'obj_id': 1, 'frame_idx': 150, 'points': [[138, 175]], 'labels': [[1]], 'box': [109, 143, 167, 208]}, {'obj_id': 1, 'frame_idx': 150, 'points': [[142, 362]], 'labels': [[1]], 'box': [115, 328, 170, 397]}, {'obj_id': 1, 'frame_idx': 150, 'points': [[141, 447]], 'labels': [[1]], 'box': [115, 400, 167, 494]}, {'obj_id': 1, 'frame_idx': 150, 'points': [[449, 751]], 'labels': [[1]], 'box': [400, 728, 499, 775]}, {'obj_id': 1, 'frame_idx': 150, 'points': [[462, 879]], 'labels': [[1]], 'box': [409, 858, 516, 900]}, {'obj_id': 1, 'frame_idx': 150, 'points': [[465, 1018]], 'labels': [[1]], 'box': [415, 996, 516, 1040]}, {'obj_id': 1, 'frame_idx': 150, 'points': [[489, 1305]], 'labels': [[1]], 'box': [428, 1258, 550, 1352]}, {'obj_id': 1, 'frame_idx': 150, 'points': [[168, 1215]], 'labels': [[1]], 'box': [142, 1158, 195, 1272]}, {'obj_id': 1, 'frame_idx': 150, 'points': [[294, 1357]], 'labels': [[1]], 'box': [232, 1314, 357, 1400]}, {'obj_id': 1, 'frame_idx': 200, 'points': [[110, 417]], 'labels': [[1]], 'box': [81, 329, 140, 506]}, {'obj_id': 1, 'frame_idx': 200, 'points': [[111, 571]], 'labels': [[1]], 'box': [87, 532, 136, 610]}, {'obj_id': 1, 'frame_idx': 200, 'points': [[112, 649]], 'labels': [[1]], 'box': [89, 588, 136, 710]}, {'obj_id': 1, 'frame_idx': 200, 'points': [[425, 1160]], 'labels': [[1]], 'box': [352, 968, 498, 1352]}, {'obj_id': 1, 'frame_idx': 250, 'points': [[405, 1301]], 'labels': [[1]], 'box': [341, 1265, 470, 1338]}, {'obj_id': 1, 'frame_idx': 250, 'points': [[404, 1238]], 'labels': [[1]], 'box': [339, 1199, 470, 1278]}, {'obj_id': 1, 'frame_idx': 250, 'points': [[404, 1164]], 'labels': [[1]], 'box': [339, 1124, 470, 1204]}, {'obj_id': 1, 'frame_idx': 250, 'points': [[404, 1096]], 'labels': [[1]], 'box': [339, 1058, 470, 1134]}, {'obj_id': 1, 'frame_idx': 250, 'points': [[404, 1026]], 'labels': [[1]], 'box': [339, 988, 470, 1064]}, {'obj_id': 1, 'frame_idx': 250, 'points': [[404, 958]], 'labels': [[1]], 'box': [339, 920, 470, 996]}, {'obj_id': 1, 'frame_idx': 250, 'points': [[404, 892]], 'labels': [[1]], 'box': [339, 854, 470, 930]}, {'obj_id': 1, 'frame_idx': 250, 'points': [[404, 826]], 'labels': [[1]], 'box': [339, 788, 470, 864]}, {'obj_id': 1, 'frame_idx': 250, 'points': [[404, 760]], 'labels': [[1]], 'box': [339, 722, 470, 798]}, {'obj_id': 1, 'frame_idx': 250, 'points': [[404, 694]], 'labels': [[1]], 'box': [339, 656, 470, 732]}, {'obj_id': 1, 'frame_idx': 250, 'points': [[404, 626]], 'labels': [[1]], 'box': [339, 589, 470, 664]}, {'obj_id': 1, 'frame_idx': 250, 'points': [[404, 561]], 'labels': [[1]], 'box': [339, 523, 470, 600]}, {'obj_id': 1, 'frame_idx': 250, 'points': [[404, 495]], 'labels': [[1]], 'box': [339, 457, 470, 533]}, {'obj_id': 1, 'frame_idx': 250, 'points': [[404, 429]], 'labels': [[1]], 'box': [339, 391, 470, 467]}, {'obj_id': 1, 'frame_idx': 250, 'points': [[404, 363]], 'labels': [[1]], 'box': [339, 325, 470, 401]}, {'obj_id': 1, 'frame_idx': 250, 'points': [[404, 297]], 'labels': [[1]], 'box': [339, 259, 470, 335]}, {'obj_id': 1, 'frame_idx': 250, 'points': [[404, 230]], 'labels': [[1]], 'box': [339, 193, 470, 267]}, {'obj_id': 1, 'frame_idx': 250, 'points': [[404, 163]], 'labels': [[1]], 'box': [339, 127, 470, 199]}, {'obj_id': 1, 'frame_idx': 250, 'points': [[404, 93]], 'labels': [[1]], 'box': [339, 60, 470, 127]}, {'obj_id': 1, 'frame_idx': 250, 'points': [[404, 30]], 'labels': [[1]], 'box': [339, 0, 470, 60]}, {'obj_id': 1, 'frame_idx': 300, 'points': [[393, 1192]], 'labels': [[1]], 'box': [340, 1156, 447, 1228]}, {'obj_id': 1, 'frame_idx': 300, 'points': [[403, 1277]], 'labels': [[1]], 'box': [340, 1239, 467, 1316]}, {'obj_id': 1, 'frame_idx': 300, 'points': [[403, 1366]], 'labels': [[1]], 'box': [340, 1328, 467, 1404]}, {'obj_id': 1, 'frame_idx': 300, 'points': [[70, 792]], 'labels': [[1]], 'box': [44, 711, 97, 874]}, {'obj_id': 1, 'frame_idx': 300, 'points': [[70, 643]], 'labels': [[1]], 'box': [44, 599, 97, 688]}, {'obj_id': 1, 'frame_idx': 300, 'points': [[70, 515]], 'labels': [[1]], 'box': [44, 471, 97, 560]}]

    # print(f"å¾—åˆ°çš„annotationsï¼š{annotations}")
    annotations = merge_annotations(annotations)
    print(f"åˆå¹¶åçš„annotationsï¼š{annotations}")
    # annotations = [{'obj_id': 7, 'frame_idx': 0, 'points': [[168, 479], [358, 479], [358, 748], [168, 748], [168, 479], [1348, 10], [1680, 10], [1680, 100], [1348, 100], [1348, 10], [1348, 130], [1680, 130], [1680, 260], [1348, 260], [1348, 130], [1348, 290], [1680, 290], [1680, 420], [1348, 420], [1348, 290], [1348, 450], [1680, 450], [1680, 580], [1348, 580], [1348, 450], [1348, 610], [1680, 610], [1680, 740], [1348, 740], [1348, 610], [1348, 770], [1680, 770], [1680, 900], [1348, 900], [1348, 770]], 'labels': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'box': []}, {'obj_id': 3, 'frame_idx': 20, 'points': [[167, 489], [388, 489], [388, 740], [167, 740], [167, 489], [1310, 168], [1680, 168], [1680, 388], [1310, 388], [1310, 168], [1310, 500], [1680, 500], [1680, 719], [1310, 719], [1310, 500]], 'labels': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'box': []}, {'obj_id': 1, 'frame_idx': 40, 'points': [[168, 479], [388, 686], [388, 1078], [168, 1078]], 'labels': [1, 1, 1, 1], 'box': []}, {'obj_id': 2, 'frame_idx': 60, 'points': [[16, 178], [498, 198], [498, 378], [16, 378], [16, 400], [418, 1088], [418, 400]], 'labels': [1, 1, 1, 1, 1, 1, 1], 'box': []}, {'obj_id': 4, 'frame_idx': 80, 'points': [[136, 187], [549, 260], [549, 378], [136, 378], [136, 580], [549, 660], [549, 1080], [136, 1080], [1308, 18], [1570, 120], [1570, 170], [1308, 170], [1200, 170], [1570, 300], [1570, 500], [1200, 500]], 'labels': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'box': []}, {'obj_id': 2, 'frame_idx': 100, 'points': [[16, 187], [598, 610], [388, 1068], [16, 187], [1200, 10], [1578, 386], [1578, 750], [1200, 10]], 'labels': [1, 1, 1, 1, 1, 1, 1, 1], 'box': []}, {'obj_id': 2, 'frame_idx': 120, 'points': [[136, 378], [590, 378], [590, 826], [136, 826], [1298, 200], [1536, 200], [1536, 576], [1298, 576]], 'labels': [1, 1, 1, 1, 1, 1, 1, 1], 'box': []}, {'obj_id': 2, 'frame_idx': 140, 'points': [[136, 478], [598, 478], [598, 886], [136, 886], [1298, 260], [1560, 260], [1560, 606], [1298, 606]], 'labels': [1, 1, 1, 1, 1, 1, 1, 1], 'box': []}, {'obj_id': 2, 'frame_idx': 160, 'points': [[136, 789], [588, 368], [588, 789], [136, 789], [1300, 340], [1596, 340], [1596, 640], [1300, 640]], 'labels': [1, 1, 1, 1, 1, 1, 1, 1], 'box': []}, {'obj_id': 3, 'frame_idx': 180, 'points': [[1368, 79], [1458, 116], [1368, 116], [1458, 79], [1340, 278], [1570, 694], [1340, 694], [1570, 278], [1, 358], [608, 996], [1, 996], [608, 358]], 'labels': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'box': []}, {'obj_id': 3, 'frame_idx': 200, 'points': [[1368, 379], [1578, 510], [1578, 688], [1368, 688], [1248, 100], [1498, 160], [1498, 240], [1248, 240], [1, 460], [598, 650], [598, 1070], [1, 1070]], 'labels': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'box': []}, {'obj_id': 3, 'frame_idx': 220, 'points': [[1368, 79], [1488, 188], [1368, 188], [1488, 79], [1308, 350], [1665, 808], [1308, 808], [1665, 350], [1, 450], [588, 1086], [1, 1086], [588, 450]], 'labels': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'box': []}, {'obj_id': 3, 'frame_idx': 240, 'points': [[1378, 69], [1498, 180], [1498, 260], [1378, 260], [1340, 390], [1686, 880], [1686, 620], [1340, 620], [0, 520], [570, 1080], [570, 520], [0, 1080]], 'labels': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'box': []}, {'obj_id': 4, 'frame_idx': 260, 'points': [[168, 79], [800, 100], [800, 200], [168, 200], [1318, 100], [1500, 100], [1500, 200], [1318, 200], [1340, 450], [1700, 450], [1700, 950], [1340, 950], [1, 560], [580, 560], [580, 1080], [1, 1080]], 'labels': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'box': []}]
    
    # --- Qwen-VL æ¨¡å‹æ¨ç†å®Œæˆï¼Œç°åœ¨é‡Šæ”¾æ˜¾å­˜ ---
    # 1. å°†æ¨¡å‹ç§»åˆ°CPUï¼ˆæˆ–ä»»ä½•å…¶ä»–ä¸ä¼šå ç”¨çš„è®¾å¤‡ï¼‰
    qwen_model.cpu() 
    
    # 2. åˆ é™¤æ¨¡å‹å¯¹è±¡åŠå…¶å¼•ç”¨
    del qwen_model 
    del qwen_processor # å¤„ç†å™¨é€šå¸¸å ç”¨è¾ƒå°‘ï¼Œä½†æœ€å¥½ä¹Ÿæ¸…ç†
    
    # 3. æ¸…ç†PyTorchçš„æ˜¾å­˜ç¼“å­˜
    # ç¡®ä¿æ‰€æœ‰æœªè¢«å¼•ç”¨çš„å¼ é‡å ç”¨çš„æ˜¾å­˜è¢«é‡Šæ”¾å›ç³»ç»Ÿ
    if torch.mlu.is_available(): # æˆ–è€… torch.cuda.is_available() for GPU
        torch.mlu.empty_cache() # æˆ–è€… torch.cuda.empty_cache()
    print("--------------------- Qwen-VL æ¨¡å‹æ˜¾å­˜å·²é‡Šæ”¾---------------------")
    # --- æ˜¾å­˜é‡Šæ”¾ç»“æŸ ---

    # 6. ä½¿ç”¨SAM2è¿›è¡Œè§†é¢‘åˆ†å‰²ï¼ˆä¸ä¹‹å‰ç›¸åŒï¼‰
    inference_state = sam_predictor.init_state(video_path=video_file)
    sam_predictor.reset_state(inference_state)
    video_segments = {}
    with torch.inference_mode(), torch.autocast("mlu", dtype=torch.float16):
        # æ·»åŠ æ‰€æœ‰æç¤º
        for annot in tqdm(annotations, desc="æ·»åŠ æç¤º"):
            # æ·»åŠ ç‚¹æç¤ºï¼ˆä»…ä¸»å®ä¾‹ï¼‰
            points_tensor = torch.tensor(annot["points"])
            # print(f"points_tensor:{points_tensor}")
            labels_tensor = torch.tensor(annot["labels"])
            # print(f"labels_tensor:{labels_tensor}")

            # æ·»åŠ æ¡†æç¤º,åç»­éœ€è¦ä¿®æ”¹
            box_tensor = torch.tensor(annot["box"][0]) if annot["box"] else None
            
            sam_predictor.add_new_points_or_box(
                inference_state=inference_state,
                frame_idx=annot["frame_idx"],
                obj_id=annot["obj_id"],
                points=points_tensor,
                labels=labels_tensor,
                box=None
            )
        
        # ä¼ æ’­å¹¶è·å–åˆ†å‰²ç»“æœ
        for out_frame_idx, out_obj_ids, out_mask_logits in sam_predictor.propagate_in_video(inference_state):
            video_segments[out_frame_idx] = {}
            for i, out_obj_id in enumerate(out_obj_ids):
                # è·å–æ©ç å¹¶ç¡®ä¿æ˜¯äºŒç»´çš„
                mask = (out_mask_logits[i] > 0.0).cpu().numpy()
                if mask.ndim == 3:  # å¦‚æœæ˜¯(1, H, W)å½¢çŠ¶
                    mask = mask[0]  # å–å‡ºç¬¬ä¸€ç»´
                elif mask.ndim > 3:  # å¦‚æœæ˜¯æ›´é«˜ç»´åº¦
                    mask = mask.squeeze()  # å‹ç¼©æ‰€æœ‰å•ç»´åº¦
                
                # ç¡®ä¿æœ€ç»ˆæ˜¯äºŒç»´çš„
                if mask.ndim != 2:
                    print(f"è­¦å‘Š: æ©ç ç»´åº¦å¼‚å¸¸ {mask.shape}ï¼Œå°è¯•è‡ªåŠ¨ä¿®æ­£")
                    mask = mask.squeeze()  # å†æ¬¡å°è¯•å‹ç¼©
                    if mask.ndim != 2:
                        raise ValueError(f"æ— æ³•ä¿®æ­£æ©ç ç»´åº¦: {mask.shape}")
                
                video_segments[out_frame_idx][out_obj_id] = mask

    # --- ï¼ˆä½¿ç”¨æŒ‡å®šé¢œè‰²ï¼‰ ---
    obj_colors = {}
    # å‡è®¾æˆ‘ä»¬é€‰æ‹©ä¸€ä¸ªæ˜äº®çš„è“è‰²
    fixed_color_bgr = (255, 0, 0) # B, G, R (è“è‰²åˆ†é‡æœ€é«˜)
    # éå†æ‰€æœ‰å¯¹è±¡IDï¼Œå¹¶ä¸ºå®ƒä»¬åˆ†é…ç›¸åŒçš„å›ºå®šé¢œè‰²
    # ä½ ä»ç„¶éœ€è¦æ”¶é›† all_obj_idsï¼Œå› ä¸ºéœ€è¦çŸ¥é“æœ‰å“ªäº›å¯¹è±¡IDä¼šå‡ºç°åœ¨ video_segments ä¸­
    all_obj_ids = set()
    for seg in video_segments.values():
        for obj_id in seg.keys():
            all_obj_ids.add(obj_id)
            
    for obj_id in all_obj_ids:
        obj_colors[obj_id] = fixed_color_bgr

    vis_frame_stride = 1
    # 7ä¿å­˜ç»“æœ
    frame_indices = range(0, total_frames, vis_frame_stride)
    image_paths = []  # å­˜å‚¨æ‰€æœ‰å›¾åƒè·¯å¾„
    log_interval = max(1, len(frame_indices) // 20)  # æ€»å…±çº¦20æ¡æ—¥å¿—

    for i, out_frame_idx in enumerate(tqdm(frame_indices, desc="Saving results")):
        frame = vr[out_frame_idx].numpy()
        output_path = os.path.join(output_dir, f"seg_{out_frame_idx:04d}.jpg")
        
        # å¦‚æœæœ‰åˆ†å‰²ç»“æœ
        if out_frame_idx in video_segments:
            masks = video_segments[out_frame_idx]
            
            # åˆ›å»ºç»“æœå¸§
            result_frame = frame.copy()
            
            # åº”ç”¨æ‰€æœ‰æ©ç 
            for obj_id, mask in masks.items():
                # æ£€æŸ¥æ©ç å½¢çŠ¶æ˜¯å¦åŒ¹é…
                if mask.shape != frame.shape[:2]:
                    print(f"è­¦å‘Š: å¸§ {out_frame_idx} çš„æ©ç å½¢çŠ¶ {mask.shape} ä¸å›¾åƒå½¢çŠ¶ {frame.shape[:2]} ä¸åŒ¹é…ï¼Œæ­£åœ¨è°ƒæ•´")
                    mask = cv2.resize(mask.astype(np.float32), 
                                    (frame.shape[1], frame.shape[0]),
                                    interpolation=cv2.INTER_NEAREST)
                    mask = mask > 0.5  # é‡æ–°äºŒå€¼åŒ–
                
                color = obj_colors[obj_id]
                
                # é«˜æ•ˆåº”ç”¨æ©ç 
                masked_area = result_frame[mask]

                if masked_area.size > 0:  # ç¡®ä¿æœ‰åŒºåŸŸéœ€è¦å¤„ç†
                    color_np = np.array(color, dtype=result_frame.dtype).reshape(1, 3)
                    blended = (masked_area * 0.7 + color_np * 0.3).astype(result_frame.dtype)
                    result_frame[mask] = blended
        
        else:
            result_frame = frame
        
        # ä¿å­˜ä¸ºJPEG
        cv2.imwrite(output_path, cv2.cvtColor(result_frame, cv2.COLOR_RGB2BGR))
        image_paths.append(output_path)
        
        # å‡å°‘æ—¥å¿—é¢‘ç‡
        # if i % log_interval == 0:
        #     print(f"Saved frame {out_frame_idx} to {output_path}")

    # åˆ›å»ºè§†é¢‘
    if image_paths:
        # åˆ›å»ºè§†é¢‘è¾“å‡ºè·¯å¾„
        video_output_path = os.path.join(output_dir, f"{CONFIG['prompts']}_result.mp4")
        
        # è·å–ç¬¬ä¸€å¼ å›¾åƒçš„å°ºå¯¸
        first_image = cv2.imread(image_paths[0])
        height, width, _ = first_image.shape
        
        # è®¾ç½®è§†é¢‘å‚æ•°
        original_fps = vr.get_avg_fps()  # è·å–åŸå§‹è§†é¢‘å¸§ç‡
        fps = max(1, original_fps // vis_frame_stride)  # è®¡ç®—è¾“å‡ºå¸§ç‡
        
        # åˆ›å»ºè§†é¢‘å†™å…¥å™¨
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # MP4ç¼–ç å™¨
        video = cv2.VideoWriter(video_output_path, fourcc, fps, (width, height))
        
        print(f"å¼€å§‹åˆæˆè§†é¢‘: {video_output_path} (å¸§ç‡: {fps}fps)")
        
        # é€å¸§æ·»åŠ å›¾åƒåˆ°è§†é¢‘
        for image_path in tqdm(image_paths, desc="åˆæˆè§†é¢‘"):
            img = cv2.imread(image_path)
            if img is not None and img.shape[:2] == (height, width):
                video.write(img)
            else:
                print(f"è­¦å‘Š: è·³è¿‡å°ºå¯¸ä¸åŒ¹é…çš„å›¾åƒ {image_path}")
        
        # é‡Šæ”¾è§†é¢‘å†™å…¥å™¨
        video.release()
        print(f"è§†é¢‘åˆæˆå®Œæˆ: {video_output_path}")
    else:
        print("æœªæ‰¾åˆ°åˆ†å‰²ç»“æœå›¾åƒï¼Œæ— æ³•åˆ›å»ºè§†é¢‘")    
    # ä¸éœ€è¦å…³é—­VideoReaderï¼Œdecordä¼šè‡ªåŠ¨ç®¡ç†èµ„æº
    # ä½†å¯ä»¥æ˜¾å¼åˆ é™¤ä»¥é‡Šæ”¾å†…å­˜
    del vr   


if __name__ == "__main__":
    main()